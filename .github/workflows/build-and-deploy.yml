name: Weekly Scrape and Deploy Princeton Academic Events

on:
  schedule:
    # Run every Sunday at 2 AM UTC (weekly scrape and rebuild)
    - cron: '0 2 * * 0'
  workflow_dispatch: # Allow manual triggering
  push:
    branches: [ master ]
    paths:
      - 'scrapers/**'
      - 'frontend/**'
      - 'functions/**'
      - 'scripts/**'

permissions:
  contents: read

jobs:
  scrape-and-deploy:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Install jq for JSON processing
      run: |
        sudo apt-get update
        sudo apt-get install -y jq

    - name: Run scrapers and aggregate data
      run: |
        echo "üï∑Ô∏è Running Python scrapers..."
        cd scrapers
        python combine_cloudscraper_events.py

    - name: Prepare static site files
      run: |
        echo "üìÅ Preparing static site files..."
        mkdir -p frontend/data

        # Copy the scraped data
        cp scrapers/all_princeton_academic_events.json frontend/data/events.json

        # Create departments.json from events data
        echo "üìä Creating departments data..."
        jq '[.events | group_by(.department) | map({name: .[0].department, meta_category: .[0].meta_category, event_count: length, is_selected: false}) | sort_by(.name)]' scrapers/all_princeton_academic_events.json > frontend/data/departments.json

        # Create a comprehensive meta file
        echo "üìã Creating meta data..."
        jq '{
          site_info: {
            name: "Princeton Academic Events",
            description: "Academic events across Princeton University",
            version: "1.0.0"
          },
          stats: {
            total_events: (.events | length),
            total_departments: (.events | group_by(.department) | length),
            last_updated: "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"
          },
          build_info: {
            build_date: "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'",
            data_source: "Princeton University Department Websites",
            update_frequency: "Weekly"
          }
        }' scrapers/all_princeton_academic_events.json > frontend/data/meta.json

    - name: Deploy to Cloudflare Pages
      uses: cloudflare/pages-action@v1
      with:
        apiToken: ${{ secrets.CLOUDFLARE_API_TOKEN }}
        accountId: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        projectName: princeton-academic-events
        directory: ./frontend
        gitHubToken: ${{ secrets.GITHUB_TOKEN }}

  notify:
    needs: scrape-and-deploy
    runs-on: ubuntu-latest
    if: always()

    steps:
    - name: Notify on success
      if: needs.scrape-and-deploy.result == 'success'
      run: |
        echo "‚úÖ Princeton Academic Events site built and deployed successfully!"
        echo "üåê Site updated with latest events data on Cloudflare Pages"

    - name: Notify on failure
      if: needs.scrape-and-deploy.result == 'failure'
      run: |
        echo "‚ùå Princeton Academic Events build failed!"
        echo "üîç Check the logs for details"
